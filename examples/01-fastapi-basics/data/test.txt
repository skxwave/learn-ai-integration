Currently (Feb 2025), LangChain has no built-in support for OpenRouter. Fortunately, OpenRouter uses its own version of the OpenAI API and translates calls made with it.

This compatibility allows us to use the ChatOpenAI class with a different endpoint and environment variable. It even supports tool calling (yay!)

Dependencies
pip install langchain_openai==0.3.7 langchain==0.3.20 python-dotenv

Code
import os
from typing import Optional

from dotenv import load_dotenv
from langchain_core.utils.utils import secret_from_env
from langchain_openai import ChatOpenAI
from pydantic import Field, SecretStr

load_dotenv()

class ChatOpenRouter(ChatOpenAI):
    openai_api_key: Optional[SecretStr] = Field(
        alias="api_key",
        default_factory=secret_from_env("OPENROUTER_API_KEY", default=None),
    )
    @property
    def lc_secrets(self) -> dict[str, str]:
        return {"openai_api_key": "OPENROUTER_API_KEY"}

    def __init__(self,
                 openai_api_key: Optional[str] = None,
                 **kwargs):
        openai_api_key = (
            openai_api_key or os.environ.get("OPENROUTER_API_KEY")
        )
        super().__init__(
            base_url="https://openrouter.ai/api/v1",
            openai_api_key=openai_api_key,
            **kwargs
        )

openrouter_model = ChatOpenRouter(
    model_name="anthropic/claude-3.7-sonnet:thinking"
)
Set the model_name argument to ChatOpenRouter to the specific model you want on OpenRouter. And set OPENROUTER_API_KEY in your .env to an API key generated via OpenRouter

openrouter_model can be used as you would with any LangChain model.
